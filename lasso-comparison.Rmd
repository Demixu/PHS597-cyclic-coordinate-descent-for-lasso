---
title: "Lasso with cyclic coordinate descent"
author: "Jingyu Xu"
date: "09/13/2020"
output:
  html_document:
    code_folding: hide
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE}
##load packages
library(tidyverse)
library(glmnet)
```

## Construct our own lasso function utilizing cyclic coordinate descent method

```{r}
##soft threshold function
soft_thre = function(a,lambda,X_square){
  adjust = 0
 if(a>lambda){
   adjust = (a-lambda)/X_square
 }
  if(a< -lambda){
    adjust = (a+lambda)/X_square
  }
  adjust
}##when we use standardized data, X square equals to 1, which can be calculated by adding up mean's square and variance.

##construct our main function using cyclic coordinate descent(ccf)
lasso_ccd <- function(
  X,                   # predictor matrix
  Y,                   # outcome value
  start_ridge,         #start point of the coefficient estimation(use the close form of ridge or not)
  num=1,                 #the start point if we do not use ridge estimation(default1)
  lambda,              # penalty parameter
  iteration,           # number of iterations
  epsilon=0,             #the difference of estimated predictor between two iterations(default=0,which means we don't use this criterion in simulation)
  intercept            #whether intercept is considered in our final model(note that intercept is not penalized in the model estimation)
) {
  if(intercept){
    intercept = rep(1,nrow(Y))
    X = as.matrix(X)
    Y = as.matrix(Y)
    X = cbind(intercept,X)
  }          #If we need to consider intercept, we should first update our design matrix
   if(start_ridge){
     beta = solve(crossprod(X) + diag(lambda, ncol(X))) %*% crossprod(X,Y)
   }
  else{
    beta = rep(num,ncol(X))
  }
  N = nrow(Y)#total number of obervations
  i=1
  dif = 11
   while (i < iteration&& dif > epsilon)#two criterion is set
     {
     ##following are the process of updating parameters
    for(j in 1: ncol(X)){
      beta_last = beta #restore the last beta to compare difference
      r_j = Y - X[,-j]%*% beta[-j]
      XY = crossprod(X[,j],r_j)/N
      X_square = crossprod(X[,j],X[,j])/N
      if ((j==1) && intercept) {
        beta[j]=XY/X_square
      }else{
      beta[j] = soft_thre(XY,lambda,X_square)}
      dif = crossprod(beta_last-beta,beta_last-beta)
      cc=j
    }  
     i=i+1
   }
  beta
}
```

## Simulate data

```{r}
set.seed(111)
Sigma <- diag(c(8,2,6,4),4,4)
Sigma[1,2]=1.5
Sigma[2,1]=1.5#X1 and x2 are correlated 
X =MASS::mvrnorm(n=100, c(2, 2,2,3), Sigma)%>%as.data.frame()
colnames(X)=c("X1","X2","X3","X4")
X1 =X[,1]
X2 =X[,2]
X3 =X[,3]
X4= X[,4]
#generate Y
eps = rnorm(100,mean=0,sd=2)
Y = 2+8*X1+0.75*X4+eps
data=data.frame(X,Y)
```

## Compare with glmnet package

Then, we campare the performance between our function and glmnet package. lambda = 0.3 is selected for comparison and the interation times is 1000.

Note that the difference between updated and former predictor is not used as a threshold there. However, for dataset with higher dimension, we could use both the change and iteration times for the ending condition.

**Without intercept**

```{r}
X = as.matrix(X)
Y = as.matrix(Y)
 lasso1 = glmnet(X,Y,alpha = 1,lambda = 0.3,
    intercept = FALSE,
    standardize = FALSE)
##lasso coefficient
coef(lasso1)
##lasso coefficient with cyclic cordinate descent(set the start points as all one)
lasso_ccd(X,Y,start_ridge = FALSE,lambda = 0.3,iteration=1000,intercept=FALSE)
##lasso coefficient with cyclic cordinate descent(set the start points as ridge estimator)
lasso_ccd(X,Y,start_ridge = TRUE,lambda = 0.3,iteration=1000,intercept=TRUE)
```
**With intercept**

```{r}
 lasso2 = glmnet(X,Y, alpha = 1,
    lambda = 0.3,
    standardize = FALSE
  )

##lasso coefficient with lasso
coef(lasso2)
##lasso coefficient with cyclic cordinate descent(set the start points as all 1)
lasso_ccd(X,Y,start_ridge = FALSE,lambda = 0.3,iteration=1000,intercept=TRUE)
##lasso coefficient with cyclic cordinate descent(set the start points as ridge estimator)
lasso_ccd(X,Y,start_ridge = TRUE,lambda = 0.3,iteration=1000,intercept=TRUE)
```

From the above, we can see the results are relevantly close. Also, the start point doesn't influence the result.
